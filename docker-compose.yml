# Compose v2+ (omit the old "version" key)

networks:
  dsnet:

volumes:
  zk_data:
  kafka_data:

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks: [dsnet]
    volumes:
      - zk_data:/var/lib/zookeeper/data
    healthcheck:
      test: ["CMD-SHELL", "zookeeper-shell localhost:2181 ls / >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 15

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      # Two listeners: in-cluster (PLAINTEXT) + host (EXTERNAL)
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      # Single-broker friendly settings
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1

      KAFKA_LOG_DIRS: /var/lib/kafka/data
    ports:
      - "9094:9094"   # host access for NiFi â†’ use localhost:9094
    networks: [dsnet]
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server 127.0.0.1:9092 >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 30

  spark-master:
    image: apache/spark:3.5.3-scala2.12-java11-python3-ubuntu
    container_name: spark-master
    restart: unless-stopped
    command: ["/opt/spark/bin/spark-class","org.apache.spark.deploy.master.Master","--host","spark-master","--port","7077","--webui-port","8080"]
    environment:
      SPARK_NO_DAEMONIZE: "true"
    ports:
      - "7077:7077"
      - "8080:8080"
    networks: [dsnet]
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./data:/data
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'exec 3<>/dev/tcp/127.0.0.1/8080 && exec 3>&- 2>&- || exit 1'"]
      interval: 10s
      timeout: 5s
      retries: 30

  spark-worker-1:
    image: apache/spark:3.5.3-scala2.12-java11-python3-ubuntu
    container_name: spark-worker-1
    restart: unless-stopped
    command: ["/opt/spark/bin/spark-class","org.apache.spark.deploy.worker.Worker","spark://spark-master:7077","--webui-port","8081"]
    environment:
      SPARK_WORKER_CORES: "2"
      SPARK_WORKER_MEMORY: "2g"
      SPARK_NO_DAEMONIZE: "true"
    ports:
      - "8081:8081"
    networks: [dsnet]
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./data:/data

  spark-worker-2:
    image: apache/spark:3.5.3-scala2.12-java11-python3-ubuntu
    container_name: spark-worker-2
    restart: unless-stopped
    command: ["/opt/spark/bin/spark-class","org.apache.spark.deploy.worker.Worker","spark://spark-master:7077","--webui-port","8082"]
    environment:
      SPARK_WORKER_CORES: "2"
      SPARK_WORKER_MEMORY: "2g"
      SPARK_NO_DAEMONIZE: "true"
    ports:
      - "8082:8082"
    networks: [dsnet]
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./data:/data
